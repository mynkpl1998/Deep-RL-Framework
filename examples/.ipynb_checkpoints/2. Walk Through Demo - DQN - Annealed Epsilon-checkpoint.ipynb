{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Deep RL - A Deep Reinforcement Learning Framework </h1>\n",
    "\n",
    "![](imgs/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the path of DeepRL\n",
    "\n",
    "Before using any component of the library we need to add the framework to the path of Python interpreter. We append the location of the root folder of the framework to system path. In my case it is at the following path. After setting the path we import the RLExp class which helps us to create a Deep RL Experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMEWORK_PATH = \"/home/mayank/Documents/Codes/ValueBased_DeepRL/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(FRAMEWORK_PATH)\n",
    "from RLExp import RLExp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying available implementations\n",
    "\n",
    "The framework is under heavy development and we are adding new algorithms after testing everyday. The list of all available implementations of algorithm, exploration strategies, experience replays can be queried as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = RLExp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get list of all available algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DQN', 'DDQN']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.getAvailableAlgorithms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get list of all implemented experience replays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Uniform-Sampling', 'Prioritized-Sampling']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.getAvailableReplays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get list of all implemented exploration strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['epsilon-greedy', 'annealing-ep-greedy']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.getAvailableExplorePolicies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up RL Experiment\n",
    "\n",
    "Now, we set up a RL experiment. To do that we need to call a function setup_exp of class RLExp which takes the following arguments as the input. In this step we will define the algorithm, exploration policy, replay method and neural network architecture we like to use for our experiment.\n",
    "\n",
    "Arguments of setup_exp function\n",
    "-  algorithm - one of the algorithm returned using getAvailableAlgorithms method\n",
    "-  explore_policy - one of the exploration policy returned using getAvailableExplorePolicies method\n",
    "-  replay - one of the replay returned using getAvailableReplays method\n",
    "-  observation_shape - shape of the observation which is feed to the input of neural network\n",
    "-  num_actions - the number of actions agent can perform in the environment\n",
    "\n",
    "Further, to demonstrate we will train our agent on a simple cartpole environment from OpenAI Gym. Since, OpenAI exposes the size of Action and Observation so it becomes really easy to determine the shape of observation and num of actions which are required as the arguments to the setup_exp method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Observation Shape :  4\n",
      "Number of actions :  Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "#env = wrappers.Monitor(env, 'data/cartpole-experiment-1')\n",
    "observation_size = env.observation_space.shape[0] # observation should be numpy array/vector\n",
    "num_actions = env.action_space\n",
    "print(\"Observation Shape : \",observation_size)\n",
    "print(\"Number of actions : \",num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call setup_exp method it looks for the defination of neural network architecture which is compatible with provided observation_shape and num_actions. The network should be defined in the file provided in \"network\" folder. Please do not change the file name. File **q_network.py** should contain the network defination. The network is defined using PyTorch. Example given below demonstrates how to define a PyTorch Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mnetwork\u001b[00m\r\n",
      "├── \u001b[01;34m__pycache__\u001b[00m\r\n",
      "│   └── q_network.cpython-36.pyc\r\n",
      "└── q_network.py\r\n",
      "\r\n",
      "1 directory, 2 files\r\n"
     ]
    }
   ],
   "source": [
    "! cd ..; tree network;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import torch.nn as nn\r\n",
      "import torch\r\n",
      "\r\n",
      "# Define your network here\r\n",
      "# input output format should be same as provided here.\r\n",
      "class q_network(nn.Module):\r\n",
      "\r\n",
      "    def __init__(self,input_size,out_size):\r\n",
      "        super(q_network,self).__init__()\r\n",
      "\r\n",
      "        self.input_size = input_size\r\n",
      "        self.out_size = out_size\r\n",
      "\r\n",
      "        self.layer1 = nn.Linear(out_features=24,in_features=self.input_size)\r\n",
      "        self.layer2 = nn.Linear(out_features=48,in_features=24)\r\n",
      "        self.layer3 = nn.Linear(out_features=self.out_size,in_features=48)\r\n",
      "\r\n",
      "        self.relu = nn.ReLU()\r\n",
      "\r\n",
      "    def forward(self,x,bsize):\r\n",
      "\r\n",
      "        x = x.view(bsize,self.input_size)\r\n",
      "        q_out = self.relu(self.layer1(x))\r\n",
      "        q_out = self.relu(self.layer2(q_out))\r\n",
      "        q_out = self.layer3(q_out)\r\n",
      "\r\n",
      "        return q_out\r\n"
     ]
    }
   ],
   "source": [
    "! cd ..; cd network; cat q_network.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling setup_exp method to create an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, once all the step given above are completed. We can call setup_exp function to create the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Set to :  cpu\n",
      "q_network(\n",
      "  (layer1): Linear(in_features=4, out_features=24, bias=True)\n",
      "  (layer2): Linear(in_features=24, out_features=48, bias=True)\n",
      "  (layer3): Linear(in_features=48, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Algorithm Setup Done. Please set hyper-parameters.\n"
     ]
    }
   ],
   "source": [
    "exp.setup_exp(algorithm=\"DQN\",explore_policy=\"annealing-ep-greedy\",replay=\"Uniform-Sampling\",observation_shape=observation_size,num_actions=2,exp_name='cartpole',save_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting hyper-parameters\n",
    "\n",
    "An RL experiment have many hyper-parameters which are required to tuned in order to get the best performance from the algorithms. The Deep RL exposes all the hyper-parameters which can be changed and queried at any time during the experiment. The following methods can be used to get the all hyper-parameters of each component, followed by how to change them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying and Setting Algorithm hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================\n",
      "Hyper Paramters (DQN)\t\t\t\t\tCurrent Value\n",
      "-------------------------------------------------------------------\n",
      "1. batch_size\t\t\t\t\t\t\tNone\n",
      "2. gamma\t\t\t\t\t\t\tNone\n",
      "3. freeze_steps\t\t\t\t\t\t\tNone\n",
      "4. max_steps\t\t\t\t\t\t\tNone\n",
      "5. max_episodes\t\t\t\t\t\t\tNone\n",
      "6. optimizer\t\t\t\t\t\t\tNone\n",
      "7. learn rate\t\t\t\t\t\t\tNone\n",
      "8. loss func\t\t\t\t\t\t\tNone\n",
      "===================================================================\n"
     ]
    }
   ],
   "source": [
    "exp.algorithm_object.getHyperParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.algorithm_object.setHyperParams(batch_size=32,gamma=0.99,freeze_steps=10000,max_steps=10**6,max_episodes=10000,optimizer='adam',lr=0.001,loss=\"huber\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying and Setting Exploration Policy hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================\n",
      "Hyper Paramters (annealed_ep_greedy)\t\t\tCurrent Value\n",
      "-------------------------------------------------------------------\n",
      "1. current_epsilon\t\t\t\t\tNone\n",
      "2. intital_epsilon\t\t\t\t\tNone\n",
      "3. final_epsilon\t\t\t\t\tNone\n",
      "4. episodes_to_anneal\t\t\t\t\tNone\n",
      "===================================================================\n"
     ]
    }
   ],
   "source": [
    "exp.explore_policy_object.getHyperParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.explore_policy_object.setHyperParams(episodes_to_anneal=7000,initial_epsilon=1.0,final_epsilon=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying and Setting Replay hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================\n",
      "Hyper Paramters (uniform-sampling)\t\t\tCurrent Value\n",
      "-------------------------------------------------------------------\n",
      "1. capacity\t\t\t\t\t\tNone\n",
      "===================================================================\n"
     ]
    }
   ],
   "source": [
    "exp.replay_object.getHyperParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.replay_object.setHyperParams(capacity=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing Final values of hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================\n",
      "Hyper Paramters (DQN)\t\t\t\t\tCurrent Value\n",
      "-------------------------------------------------------------------\n",
      "1. batch_size\t\t\t\t\t\t\t32\n",
      "2. gamma\t\t\t\t\t\t\t0.99\n",
      "3. freeze_steps\t\t\t\t\t\t\t10000\n",
      "4. max_steps\t\t\t\t\t\t\t1000000\n",
      "5. max_episodes\t\t\t\t\t\t\t10000\n",
      "6. optimizer\t\t\t\t\t\t\tadam\n",
      "7. learn rate\t\t\t\t\t\t\t0.001\n",
      "8. loss func\t\t\t\t\t\t\thuber\n",
      "===================================================================\n"
     ]
    }
   ],
   "source": [
    "exp.algorithm_object.getHyperParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================\n",
      "Hyper Paramters (annealed_ep_greedy)\t\t\tCurrent Value\n",
      "-------------------------------------------------------------------\n",
      "1. current_epsilon\t\t\t\t\t1.0\n",
      "2. intital_epsilon\t\t\t\t\t1.0\n",
      "3. final_epsilon\t\t\t\t\t0.01\n",
      "4. episodes_to_anneal\t\t\t\t\t7000.0\n",
      "===================================================================\n"
     ]
    }
   ],
   "source": [
    "exp.explore_policy_object.getHyperParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================\n",
      "Hyper Paramters (uniform-sampling)\t\t\tCurrent Value\n",
      "-------------------------------------------------------------------\n",
      "1. capacity\t\t\t\t\t\t1000000\n",
      "===================================================================\n"
     ]
    }
   ],
   "source": [
    "exp.replay_object.getHyperParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "Since, we give the control of looping over loop to the programmer. He/She need to pass the current step and episode number whenever calling the RLExp class so that the data can be logged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill the memory with random transitions\n",
    "\n",
    "Filling the replay memory before running the training loop is a good idea and hence, we force use to fill the replay to atleast greater than batch size. If replay memory is smaller than the batch size an error is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(0,1000):\n",
    "    \n",
    "    prev_state = env.reset()\n",
    "    \n",
    "    for step in range(0,exp.algorithm_object.params[\"max_steps\"]):\n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        \n",
    "        sample = exp.pack_sample(prev_state,action,reward,next_state,done)\n",
    "        \n",
    "        exp.replay_object.add_sample(sample)\n",
    "        \n",
    "        prev_state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Note\n",
    "\n",
    "Please don't rerun this block of the code if it is once executed. Restart from start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epsiode in range(0,exp.algorithm_object.params[\"max_episodes\"]):\n",
    "    \n",
    "    episode_reward = 0.0\n",
    "    prev_state = env.reset()\n",
    "    #env.render()\n",
    "    \n",
    "    for step in range(0,exp.algorithm_object.params[\"max_steps\"]):\n",
    "        \n",
    "        # get an action\n",
    "        action = exp.explore_policy_object.exploreAction(state=prev_state,curr_epsiode=epsiode)\n",
    "        \n",
    "        # perform action in environment\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        \n",
    "        # pack the data into a named tuple provided by the framework\n",
    "        sample = exp.pack_sample(prev_state,action,reward,next_state,done)\n",
    "        \n",
    "        # add sequence to experience replay\n",
    "        exp.replay_object.add_sample(sample)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        prev_state = next_state\n",
    "        \n",
    "        exp.algorithm_object.update()\n",
    "        \n",
    "        # important\n",
    "        exp.time_steps_elapsed += 1\n",
    "        \n",
    "        # copy weights\n",
    "        if (exp.time_steps_elapsed % exp.algorithm_object.params[\"freeze_steps\"]) == 0:\n",
    "            exp.algorithm_object.copytoTarget()\n",
    "        \n",
    "        # update epsilon\n",
    "        exp.explore_policy_object.updateEpsilon()\n",
    "        \n",
    "        if done:\n",
    "            #print(done,step+1)\n",
    "            break\n",
    "    \n",
    "    # store episode reward\n",
    "    exp.algorithm_object.store_episode_reward(episode_reward)\n",
    "    \n",
    "    \n",
    "    if (epsiode+1) % exp.save_interval == 0:\n",
    "        exp.dumpData_object.export_csv()\n",
    "    \n",
    "    print('Current Exploration Rate : %.4f'%(exp.explore_policy_object.params[\"current_epsilon\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Performance of the agent\n",
    "test_episodes = 100\n",
    "reward_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Done\n"
     ]
    }
   ],
   "source": [
    "for episode in range(0,test_episodes):\n",
    "    \n",
    "    prev_state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(0,1000):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            torch_x = torch.from_numpy(prev_state).to(exp.device).float()\n",
    "            out = exp.main_model.forward(torch_x,bsize=1)\n",
    "            val, action = out.max(dim=1)\n",
    "            action = int(action)\n",
    "        \n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        prev_state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    reward_list.append(total_reward)\n",
    "print('Test Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward :  94.61\n",
      "Std. Deviation :  3.158781410607578\n",
      "Max Reward :  103.0\n",
      "Min Reward :  88.0\n"
     ]
    }
   ],
   "source": [
    "reward_list = np.array(reward_list)\n",
    "print('Mean Reward : ',reward_list.mean())\n",
    "print('Std. Deviation : ',reward_list.std())\n",
    "print('Max Reward : ',reward_list.max())\n",
    "print('Min Reward : ',reward_list.min())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
